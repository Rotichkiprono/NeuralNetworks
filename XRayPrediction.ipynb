{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9681947,
          "sourceType": "datasetVersion",
          "datasetId": 5918106
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Chest X-Ray Prediction - On Going Work",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Here are the default libraries imported from Kaggle to let us work on the dataset in the tab \"Input.\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-output": true,
        "id": "nD2_h8HdpaXp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Preprocessing"
      ],
      "metadata": {
        "id": "uNgPU1lppaXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing some more libraries\n",
        "!pip install tqdm\n",
        "\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T05:48:46.487962Z",
          "iopub.execute_input": "2024-11-30T05:48:46.49004Z",
          "iopub.status.idle": "2024-11-30T05:49:01.287467Z",
          "shell.execute_reply.started": "2024-11-30T05:48:46.489983Z",
          "shell.execute_reply": "2024-11-30T05:49:01.285486Z"
        },
        "id": "_oYp_J8CpaXq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Reorganizing the dataset into an array of image_path and label to split the images for training and testing.\n",
        "The result should be a simple table with two columns (image path, label)\n",
        "At the end, I print the first 5 rows to confirm the data.\n",
        "I display a sample of X-Ray for each category.\n",
        "'''\n",
        "\n",
        "# Define base path\n",
        "base_path = '/kaggle/input/chest-x-ray-dataset-4-categories/Chest X_Ray Dataset'\n",
        "\n",
        "# Reorganize the dataset\n",
        "data = []\n",
        "for label in os.listdir(base_path):\n",
        "    label_path = os.path.join(base_path, label)\n",
        "    if os.path.isdir(label_path):\n",
        "        for file_name in os.listdir(label_path):\n",
        "            file_path = os.path.join(label_path, file_name)\n",
        "            if os.path.isfile(file_path):\n",
        "                data.append({'image_path': file_path, 'label': label})\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "\n",
        "# Select one sample per category\n",
        "unique_labels = df['label'].unique()\n",
        "sample_images = df.groupby('label').first().reset_index()\n",
        "\n",
        "# Plot one image per category\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, row in sample_images.iterrows():\n",
        "    image_path = row['image_path']\n",
        "    label = row['label']\n",
        "    image = mpimg.imread(image_path)\n",
        "\n",
        "    plt.subplot(1, len(sample_images), i + 1)\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title(label)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T05:01:59.926786Z",
          "iopub.execute_input": "2024-11-30T05:01:59.927312Z",
          "iopub.status.idle": "2024-11-30T05:02:12.270055Z",
          "shell.execute_reply.started": "2024-11-30T05:01:59.927267Z",
          "shell.execute_reply": "2024-11-30T05:02:12.268567Z"
        },
        "id": "Oj58JBMupaXr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Now we split the Dataframe into training data, validation data (used for the training process), and the testing data (to test our model).\n",
        "We split first 70% of the data to the train dataset (train_df), and 30% to temporary dataset (temp_def, used for the test and validation).\n",
        "Then, we split the temporary dataset in half for the test data (test_df) and validation data (val_df).\n",
        "In the end, we have 70% training images (train_df), 15% validation images (val_df), and 15% testing images (test_df).\n",
        "I print the amount of images for each category to confirm the sizes.\n",
        "'''\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
        "\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
        "\n",
        "print(f\"Training samples: {len(train_df)}\")\n",
        "print(f\"Validation samples: {len(val_df)}\")\n",
        "print(f\"Test samples: {len(test_df)}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T02:42:53.043765Z",
          "iopub.status.idle": "2024-11-30T02:42:53.045493Z",
          "shell.execute_reply.started": "2024-11-30T02:42:53.044688Z",
          "shell.execute_reply": "2024-11-30T02:42:53.044772Z"
        },
        "id": "NgYe9xDxpaXr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "My dataset has much more data for pneumonia and normal X_ray samples. This is an issue in Machine Learning because the model will train\n",
        "more often on these samples. Which will lead him to be less performant for the Covid and Tuberculosis samples. To solve this issue I use oversampling\n",
        "technique. I duplicate the Covid and Tuberculosis images for the model to see these images equally with other samples.\n",
        "To do so I need to copy the image, assign a path name to the duplicate and save it in a dedicated file (oversampled_data).\n",
        "At the end I print the number of images for each illness to confirm we balanced properly our data which is 3000 images for each illness.\n",
        "Note that we do this only for the training data.\n",
        "'''\n",
        "\n",
        "def oversample_training_set(df, target_count_per_class, output_dir):\n",
        "    \"\"\"\n",
        "    Oversample the dataset to ensure balanced classes and save duplicate files in a writable directory.\n",
        "    \"\"\"\n",
        "    oversampled_data = []\n",
        "\n",
        "    for label in df['label'].unique():\n",
        "        class_subset = df[df['label'] == label]\n",
        "        oversample_count = target_count_per_class - len(class_subset)\n",
        "\n",
        "        # Add original data\n",
        "        oversampled_data.extend(class_subset.to_dict(orient='records'))\n",
        "\n",
        "        # Create duplicates in the writable output directory\n",
        "        if oversample_count > 0:\n",
        "            label_dir = os.path.join(output_dir, label)\n",
        "            os.makedirs(label_dir, exist_ok=True)  # Ensure directory exists\n",
        "\n",
        "            for _, row in class_subset.sample(oversample_count, replace=True, random_state=42).iterrows():\n",
        "                original_path = row['image_path']\n",
        "\n",
        "                # Generate a unique name for the duplicate\n",
        "                new_file_name = f\"{os.path.splitext(os.path.basename(original_path))[0]}_dup_{len(oversampled_data)}{os.path.splitext(original_path)[1]}\"\n",
        "                new_file_path = os.path.join(label_dir, new_file_name)\n",
        "\n",
        "                # Physically copy the file\n",
        "                shutil.copy(original_path, new_file_path)\n",
        "\n",
        "                # Add the duplicate to the DataFrame\n",
        "                new_row = row.copy()\n",
        "                new_row['image_path'] = new_file_path\n",
        "                oversampled_data.append(new_row)\n",
        "\n",
        "    return pd.DataFrame(oversampled_data)\n",
        "\n",
        "\n",
        "# Define output directory for writable files\n",
        "output_dir = '/kaggle/working/oversampled_data'\n",
        "\n",
        "# Oversample training set\n",
        "train_df_balanced = oversample_training_set(train_df, target_count_per_class=3000, output_dir=output_dir)\n",
        "\n",
        "# Check the new training set distribution\n",
        "print(train_df_balanced['label'].value_counts())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T02:10:56.138434Z",
          "iopub.execute_input": "2024-11-30T02:10:56.139587Z",
          "iopub.status.idle": "2024-11-30T02:11:11.235722Z",
          "shell.execute_reply.started": "2024-11-30T02:10:56.139476Z",
          "shell.execute_reply": "2024-11-30T02:11:11.234557Z"
        },
        "id": "Qc_gjghqpaXs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Now, it is time to organize my directory. I put my images in test, train and validation (val) folders.\n",
        "The MobileNetV2 model is expecting the images to be organized this way.\n",
        "'''\n",
        "\n",
        "def organize_dataset_with_progress(df, base_output_path):\n",
        "    \"\"\"\n",
        "    Organizes the dataset into directories for train, validation, and test,\n",
        "    copying all files into the appropriate folders.\n",
        "    \"\"\"\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {base_output_path}\"):\n",
        "        label = row['label']\n",
        "        src_path = row['image_path']  # Use the correct column\n",
        "        dest_dir = os.path.join(base_output_path, label)\n",
        "        os.makedirs(dest_dir, exist_ok=True)  # Ensure the directory exists\n",
        "        dest_path = os.path.join(dest_dir, os.path.basename(src_path))\n",
        "\n",
        "        try:\n",
        "            shutil.copy(src_path, dest_path)  # Copy the file\n",
        "        except Exception as e:\n",
        "            print(f\"Error copying {src_path} to {dest_path}: {e}\")\n",
        "\n",
        "# Define output paths\n",
        "train_path = '/kaggle/working/train'\n",
        "val_path = '/kaggle/working/val'\n",
        "test_path = '/kaggle/working/test'\n",
        "\n",
        "# Remove the train, val, and test folders completely\n",
        "for folder in [train_path, val_path, test_path]:\n",
        "    if os.path.exists(folder):\n",
        "        shutil.rmtree(folder)\n",
        "\n",
        "# Organize datasets with a progress bar\n",
        "organize_dataset_with_progress(train_df_balanced, train_path)\n",
        "organize_dataset_with_progress(val_df, val_path)\n",
        "organize_dataset_with_progress(test_df, test_path)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T02:17:46.531029Z",
          "iopub.execute_input": "2024-11-30T02:17:46.531483Z",
          "iopub.status.idle": "2024-11-30T02:18:12.273688Z",
          "shell.execute_reply.started": "2024-11-30T02:17:46.531443Z",
          "shell.execute_reply": "2024-11-30T02:18:12.272462Z"
        },
        "id": "16rzyGL5paXs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def display_samples_from_folders(base_paths):\n",
        "    \"\"\"\n",
        "    Displays one sample image from each subfolder (label category) in the given base paths.\n",
        "    \"\"\"\n",
        "    for base_path in base_paths:\n",
        "        print(f\"\\nDisplaying samples from: {base_path}\")\n",
        "        categories = os.listdir(base_path)\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        for i, category in enumerate(categories):\n",
        "            category_path = os.path.join(base_path, category)\n",
        "            if os.path.isdir(category_path):\n",
        "                # Get the first image in the category\n",
        "                first_image = os.listdir(category_path)[0]\n",
        "                first_image_path = os.path.join(category_path, first_image)\n",
        "                # Read and plot the image\n",
        "                image = mpimg.imread(first_image_path)\n",
        "                plt.subplot(1, len(categories), i + 1)\n",
        "                plt.imshow(image, cmap='gray')\n",
        "                plt.title(category)\n",
        "                plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Define base paths\n",
        "base_paths = [train_path, val_path, test_path]\n",
        "\n",
        "# Display samples from train, val, and test folders\n",
        "display_samples_from_folders(base_paths)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T05:07:05.015211Z",
          "iopub.execute_input": "2024-11-30T05:07:05.015789Z",
          "iopub.status.idle": "2024-11-30T05:07:09.699523Z",
          "shell.execute_reply.started": "2024-11-30T05:07:05.015741Z",
          "shell.execute_reply": "2024-11-30T05:07:09.698282Z"
        },
        "id": "fv7cBTHbpaXt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "I assign the folders path to their value to use them into the model.\n",
        "I also create mulitple angles of the same image to increase the number of images for the model to train.\n",
        "'''\n",
        "\n",
        "\n",
        "# Paths to the train, validation, and test directories\n",
        "train_path = '/kaggle/working/train'\n",
        "val_path = '/kaggle/working/val'\n",
        "test_path = '/kaggle/working/test'\n",
        "\n",
        "# Training data generator with data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,            # Normalize pixel values to [0, 1]\n",
        "    rotation_range=40,         # Random rotation\n",
        "    width_shift_range=0.2,     # Random horizontal shift\n",
        "    height_shift_range=0.2,    # Random vertical shift\n",
        "    shear_range=0.2,           # Random shear transformation\n",
        "    zoom_range=0.2,            # Random zoom\n",
        "    horizontal_flip=True,      # Random horizontal flip\n",
        "    fill_mode='nearest'        # Fill missing pixels after transformations\n",
        ")\n",
        "\n",
        "# Validation and test data generators (no augmentation)\n",
        "val_test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255  # Normalize pixel values to [0, 1]\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T02:22:31.150117Z",
          "iopub.execute_input": "2024-11-30T02:22:31.150705Z",
          "iopub.status.idle": "2024-11-30T02:22:31.157321Z",
          "shell.execute_reply.started": "2024-11-30T02:22:31.150653Z",
          "shell.execute_reply": "2024-11-30T02:22:31.156099Z"
        },
        "id": "Jc10xHlcpaXt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(224, 224),  # Resize images to match input size of the model\n",
        "    batch_size=32,           # Number of images per batch\n",
        "    class_mode='categorical' # Multi-class classification\n",
        ")\n",
        "\n",
        "# Validation data generator\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    val_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Test data generator\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    test_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False  # No shuffling for test data to ensure reproducibility\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T02:22:35.312299Z",
          "iopub.execute_input": "2024-11-30T02:22:35.312758Z",
          "iopub.status.idle": "2024-11-30T02:22:35.670447Z",
          "shell.execute_reply.started": "2024-11-30T02:22:35.312714Z",
          "shell.execute_reply": "2024-11-30T02:22:35.66918Z"
        },
        "id": "6huszv3cpaXt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define a single image for augmentation\n",
        "# Reset the generator to fetch images from the start\n",
        "train_generator.reset()\n",
        "original_images, labels = next(train_generator)  # Fetch a single batch\n",
        "original_image = original_images[0]  # Take the first image in the batch\n",
        "\n",
        "# Generate multiple augmented versions of the same image\n",
        "augmented_images = []\n",
        "for _ in range(5):  # Generate 5 augmented images\n",
        "    augmented_image, _ = next(train_generator)\n",
        "    augmented_images.append(augmented_image[0])  # Take the augmented version of the first image\n",
        "\n",
        "# Plot the original image alongside the augmented images\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Plot the original image\n",
        "plt.subplot(1, 6, 1)\n",
        "plt.imshow(original_image)\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis('off')\n",
        "\n",
        "# Plot augmented images\n",
        "for i, aug_image in enumerate(augmented_images):\n",
        "    plt.subplot(1, 6, i + 2)\n",
        "    plt.imshow(aug_image)\n",
        "    plt.title(f\"Augmented {i + 1}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T04:58:35.043283Z",
          "iopub.execute_input": "2024-11-30T04:58:35.043877Z",
          "iopub.status.idle": "2024-11-30T04:58:41.588235Z",
          "shell.execute_reply.started": "2024-11-30T04:58:35.043829Z",
          "shell.execute_reply": "2024-11-30T04:58:41.586874Z"
        },
        "id": "s3M-IiFkpaXu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "osRhbKCfpaXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the base model\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Learning rate scheduler to reduce learning rate on plateau\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1\n",
        ")\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', patience=5, restore_best_weights=True, verbose=1\n",
        ")\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "\n",
        "# Convert to dictionary\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "\n",
        "# Unfreeze the last 50 layers\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-50]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification layers on top\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)  # Global average pooling\n",
        "x = Dropout(0.6)(x)              # Dropout for regularization\n",
        "x = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(x)  # Fully connected layer with L2\n",
        "predictions = Dense(4, activation='softmax')(x)  # Output layer (4 classes)\n",
        "\n",
        "# Final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [lr_scheduler, early_stopping]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    validation_steps=val_generator.samples // val_generator.batch_size,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T03:30:26.937103Z",
          "iopub.execute_input": "2024-11-30T03:30:26.937629Z",
          "iopub.status.idle": "2024-11-30T04:22:44.786293Z",
          "shell.execute_reply.started": "2024-11-30T03:30:26.937582Z",
          "shell.execute_reply": "2024-11-30T04:22:44.7845Z"
        },
        "id": "If_2TJAMpaXu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Testing"
      ],
      "metadata": {
        "id": "B0PNePX6paXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all test images are used\n",
        "steps = np.ceil(test_generator.samples / test_generator.batch_size).astype(int)\n",
        "\n",
        "# Print the number of test images and steps\n",
        "print(f\"Total number of test images: {test_generator.samples}\")\n",
        "print(f\"Number of steps used for evaluation: {steps}\")\n",
        "print(f\"Batch size: {test_generator.batch_size}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_generator, steps=steps, verbose=1)\n",
        "\n",
        "# Calculate error rate\n",
        "error_rate = 1 - test_accuracy\n",
        "\n",
        "# Print results\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(f\"Test Error Rate: {error_rate}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T04:56:39.634607Z",
          "iopub.execute_input": "2024-11-30T04:56:39.635133Z",
          "iopub.status.idle": "2024-11-30T04:57:22.203458Z",
          "shell.execute_reply.started": "2024-11-30T04:56:39.635092Z",
          "shell.execute_reply": "2024-11-30T04:57:22.201604Z"
        },
        "id": "Z1eKbYgppaXv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all test images are used\n",
        "steps = np.ceil(test_generator.samples / test_generator.batch_size).astype(int)\n",
        "images_used = steps * test_generator.batch_size\n",
        "print(f\"Number of images being used in this evaluation: {images_used}\")\n",
        "\n",
        "# Get predictions for all test images\n",
        "predictions = model.predict(test_generator, steps=steps, verbose=1)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "\n",
        "# Get true labels for the full test set\n",
        "true_labels = test_generator.classes\n",
        "\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(true_labels, predicted_labels, target_names=list(test_generator.class_indices.keys())))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T04:54:46.827698Z",
          "iopub.execute_input": "2024-11-30T04:54:46.828202Z",
          "iopub.status.idle": "2024-11-30T04:55:21.124618Z",
          "shell.execute_reply.started": "2024-11-30T04:54:46.82816Z",
          "shell.execute_reply": "2024-11-30T04:55:21.123358Z"
        },
        "id": "Wm55F8_xpaXv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "cm_labels = list(test_generator.class_indices.keys())\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=cm_labels, yticklabels=cm_labels)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T04:41:09.818332Z",
          "iopub.execute_input": "2024-11-30T04:41:09.818783Z",
          "iopub.status.idle": "2024-11-30T04:41:10.852251Z",
          "shell.execute_reply.started": "2024-11-30T04:41:09.818743Z",
          "shell.execute_reply": "2024-11-30T04:41:10.850973Z"
        },
        "id": "8f9YSa8IpaXv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch multiple batches to collect enough test data\n",
        "all_images = []\n",
        "all_true_labels = []\n",
        "\n",
        "# Collect all images and labels from the generator\n",
        "for i in range(len(test_generator)):\n",
        "    images, true_labels = next(test_generator)\n",
        "    all_images.append(images)\n",
        "    all_true_labels.append(true_labels)\n",
        "\n",
        "# Concatenate all batches into a single array\n",
        "all_images = np.concatenate(all_images, axis=0)\n",
        "all_true_labels = np.concatenate(all_true_labels, axis=0)\n",
        "\n",
        "# Predict labels for all test images\n",
        "predictions = model.predict(all_images)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Decode true labels from one-hot encoding\n",
        "true_labels = np.argmax(all_true_labels, axis=1)\n",
        "\n",
        "# Decode class indices\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "\n",
        "# Select 15 random indices\n",
        "random_indices = np.random.choice(len(all_images), size=15, replace=False)\n",
        "\n",
        "# Plot random images with their predictions\n",
        "plt.figure(figsize=(20, 10))\n",
        "for i, idx in enumerate(random_indices):\n",
        "    plt.subplot(3, 5, i + 1)  # 3 rows and 5 columns\n",
        "    plt.imshow(all_images[idx])\n",
        "    true_label = class_labels[true_labels[idx]]\n",
        "    predicted_label = class_labels[predicted_labels[idx]]\n",
        "    plt.title(f\"True: {true_label}\\nPred: {predicted_label}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-30T04:46:45.546619Z",
          "iopub.execute_input": "2024-11-30T04:46:45.547065Z",
          "iopub.status.idle": "2024-11-30T04:48:03.775869Z",
          "shell.execute_reply.started": "2024-11-30T04:46:45.547029Z",
          "shell.execute_reply": "2024-11-30T04:48:03.774506Z"
        },
        "id": "EvE4wCEGpaXv"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}